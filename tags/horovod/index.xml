<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Horovod on Jing Blog</title>
    <link>https://shanyit.github.io/tags/horovod/</link>
    <description>Recent content in Horovod on Jing Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Jan 2020 11:29:54 +0800</lastBuildDate>
    
	<atom:link href="https://shanyit.github.io/tags/horovod/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Gradient Checkpointing Horovod</title>
      <link>https://shanyit.github.io/post/2019-12-01-distributedtraining/</link>
      <pubDate>Thu, 02 Jan 2020 11:29:54 +0800</pubDate>
      
      <guid>https://shanyit.github.io/post/2019-12-01-distributedtraining/</guid>
      <description>GradientCheckpointingHorovod Introduction Bert, Roberta, and Xlnet models are too large to be trained. They are often partioned on several GPUs (one defines this as Model Parallelism). Actually, layers are dependent which means current layer needs the ouput of preceding layer to continue some calculation. Assuming a large model is splitted into 4 parts each on a single GPU, other parts are waitting for others when one of part is runing some computation.</description>
    </item>
    
  </channel>
</rss>